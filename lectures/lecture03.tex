\section{Newton method}

Today the main topic of our lecture is the Newton method. We will start with recalling different matrix decompositions and then move to the Newton method.

\subsection{Standard Matrix Decompositions}

We are up to solving a general system of linear equations: 
\[
A \vec{x} = \vec{b}
\]

Depending on the shape of the matrix $A$: 
\begin{itemize}
    \item If $A \in \mathbb{R}^{n \times n}, \det(A) \neq 0$, then $x = A^{-1}b$.
    \item If $A \in \mathbb{R}^{m \times n}, m > n$, then $x = (A^T A)^{-1} A^T b$.
    \item Otherwise: $\norm{A \vec{x} - \vec{b}}^2 \to \min_x$.
\end{itemize}

However, if you take a look at, e.g. \texttt{numpy} library implementation, you will not see solutions of linear systems implemented in the same terms as above. This would have been to slow. Instead, matrix decompositions are used to minimize the number of operations.

\subsubsection{Cholesky Decomposition}

\[
    A^T = A \succ 0 \Rightarrow A = L L^T, \text{ where } L \text{ is a lower triangular matrix}
\]

\notice \; I will use $\succ$ here to denote positive definiteness of a matrix.

\[
    A x = b \Longrightarrow L L^T x = b \Longleftrightarrow \begin{cases}
        y = L^{-1} b \\
        x = L^{-T} y
    \end{cases}
\]

Computing the decomposition is $\mathcal{O}(\frac{n^3}{3})$, and solving the system is $\mathcal{O}(n^2)$.

\subsubsection{Modified Cholesky Decomposition}

\[
    A^T = A \cancel{\succ} 0 \Rightarrow P^T A P = L D L^T
\]

Where L is a lower triangular matrix, and D is a block diagonal matrix. 

\notice \; Blocks in $D$ are $1 \times 1$ or $2 \times 2$.

\notice \; Matrix $P$ is called a permutation matrix. It has exactly one non-zero element in each row and each column, and this element is equal to 1.

If we do such decomposition, what will be the solution of the system?

\[ 
    A x = b \Longrightarrow P L D L^T P^T x = b \Longleftrightarrow \begin{cases}
        z = P L^{-T} b \\
        y = D^{-1} z \\
        x = L^{-1} P^T y
    \end{cases}
\]

Computing the decomposition is $\mathcal{O}(\frac{n^3}{3})$, and solving the system is $\mathcal{O}(n^2)$.

\subsubsection{LU Decomposition}

\[
    A \in \R^{m \times n} \Rightarrow P A = L U, L \in \R^{m \times \min(m, n)}, U \in \R^{\min(m, n) \times n}
\]

Where $L$ is a lower triangular matrix, and $U$ is an upper triangular matrix.

\notice \; Computational complexity is 2 times more than Cholesky decomposition, but this decomposition doesn't require positive definiteness of the matrix. Complexity is $\mathcal{O}(\frac{2}{3} n^3)$.

\subsubsection{QR Decomposition}

\[
    A \in \R^{m \times n}, m \geq n \Rightarrow A P = Q R, Q \in \R^{m \times m}, R \in \R^{m \times n}
\]

Where $Q$ is an orthogonal matrix, and $R$ is an upper triangular matrix.

\notice \; Computational complexity is $\mathcal{O}(\frac{4}{3} n^3)$.

How we solve the system with this decomposition?

\[ 
    x = (A^T A)^{-1} A^T b = (R^T Q^T Q R)^{-1} R^T Q^T b = (R^T R)^{-1} R^T Q^T b = R^{-1} R^{-T} R^T Q^T b = R^{-1} Q^T b
\] 

\notice \; $\mathcal{O}(n^2)$ complexity.

\subsubsection{Eigenvalue Decomposition}

\[
    A^T = A \Rightarrow A = Q \Lambda Q^T, Q^T Q = I
\]

Where $Q$ is an orthogonal matrix, and $\Lambda$ is a diagonal matrix with eigenvalues of $A$ on the diagonal.

\notice \; This is a very expensive decomposition, with 20 times more complexity than Cholesky decomposition. 

\subsection{Newton Method}

The basic optimization routine is: 
\[ 
    x_{k+1} = x_k + \alpha_k d_k
\] 

We have a function $f(x)$, computed at the point $x_k$, and we want to minimize it. Let's expand the function at the point $x_k$ using the Taylor series until the second order:
\[
    f(x) \approx f(x_k) + \nabla f(x_k)^T (x - x_k) + \frac{1}{2} (x - x_k)^T \nabla^2 f(x_k) (x - x_k)
\] 

To find where this function is minimized, we need to find the gradient of the function and set it to zero: 
\[ 
    \frac{\partial f(x)}{\partial x} = \nabla f(x_k) + \nabla^2 f(x_k) (x - x_k) = 0 
\] 

When we solve for $x$ the equation above, we will get the new point, at which the approximation of the function is minimized. So its logical, that it would be our new point $x_{k+1}$.

\begin{align*}
    \nabla f(x_k) + \nabla^2 f(x_k) (x_{k+1} - x_k) &= 0 \\ 
    \nabla f(x_k) + \nabla^2 f(x_k) \cdot x_{k+1} - \nabla^2 f(x_k) \cdot x_k &= 0 \\
    \nabla^2 f(x_k) \cdot x_{k+1} &= - \nabla f(x_k) + \nabla^2 f(x_k) \cdot x_k \\
    x_{k+1} &= x_k - (\nabla^2 f(x_k))^{-1} \nabla f(x_k)
\end{align*}

Newton method iteration is: 
\[ 
    x_{k+1} = x_k - \alpha_k (\nabla^2 f(x_k))^{-1} \nabla f(x_k)
\]

To be able to solve this system, we need to require that the Hessian is positive definite. 

If this condition is satisfied, we can garantuee, that: 
\[
    \nabla f(x_{k+1})^T d_k = - \nabla f(x_{k+1})^T (\nabla^2 f(x_k))^{-1} \nabla f(x_k) < 0
\]
This means that the direction is the negative gradient, meaning that the function is decreasing. If Hessian is not positive definite, we can't garantuee that we result in a descent method. 

\begin{theorem}
    Newton method doesn't struggle from poorly scaled functions, like gradient descent does. 
\end{theorem}
\begin{proof}
    Say we have a function: 
    \[
        f(x) \to \min_x 
    \]

    Lets say it has form of an ellipsoid, a circular, good one, not ellongated. Then we reparametrize the function with a linear transformation. And let's see what happens if we apply newton method to the reparametrized and the original function.

    \[
        x = A \tilde{x} \quad \tilde{x} = A^{-1} x \quad \tilde{f} (\tilde{x}) = f(A \tilde{x}) 
    \]

    To apply the Newton method, we need the gradient and the Hessian of the function $\tilde{f}$.
    \[
        d \tilde{f} = \nabla f (A \tilde{x})^T A d \tilde{x} \Longrightarrow \nabla \tilde{f} (\tilde{x}) = A^T \nabla f (A \tilde{x})
    \]

    And the Hessian:
    \begin{align*}
        d^2 \tilde{f} &= d (\nabla f (A \tilde{x})^T A d \tilde{x_1}) \\ 
        &= (\nabla^2 f (A \tilde{x}) A d \tilde{x_2})^T A d \tilde{x_1} \\ 
        &= d \tilde{x_1}^T A^T \nabla^2 f (A \tilde{x}) A d \tilde{x_1} \\ 
        &\Longrightarrow \nabla^2 \tilde{f} (\tilde{x}) \\ 
        &= A^T \nabla^2 f (A \tilde{x}) A
    \end{align*}

    So we can apply the Newton method to the function $\tilde{f}$.
    \[
        \tilde{x}_{k+1} = \tilde{x}_k - \alpha_k (\nabla^2 \tilde{f} (\tilde{x}_k))^{-1} \nabla \tilde{f} (\tilde{x}_k) = \tilde{x}_k - \alpha_k A^{-1} (\nabla^2 f (A \tilde{x}_k))^{-1} A^{-T} A^{T} \nabla f (A \tilde{x}_k)
    \]

    And in the original coordinates: 
    \[ 
        x_{k+1} = x_k - \alpha_k (\nabla^2 f (x_k))^{-1} \nabla f (x_k)
    \]

    What do we get? The step is the same, but multiplied by the matrix $A$. This means that the Newton method is invariant to linear transformations of the space and does not struggle from poorly scaled functions.
\end{proof}

\subsection{Convergence of the Newton method}

\subsubsection{Global convergence rate}

We will use a general result, that we have already deduced: 

\[
    x_{k+1} = x_k + \alpha_k d_k \quad \nabla f(x_{k})^T d_k < 0 
\]

What does it mean for the point to be close to the minimum? These are Armijo and Wolfe conditions.
\begin{itemize}
    \item $f(x_k + \alpha_k d_k) \leqslant f(x_k) + c_1 \alpha_k \nabla f(x_k)^T d_k$
    \item $\nabla f(x_k + \alpha_k d_k) \leqslant \nabla f(x_k)^T d_k$
\end{itemize}

From the previous lecture, we have deduced this condition on the convergence rate:  
\[ 
    f(x_{k+1}) \leqslant f(x_k) - \frac{c_1 (1 - c_2)}{L} \cos^2 \theta_k \norm{\nabla f(x_k)}^2
\] 

Here we only need to prove that $\cos \theta_k$ is bounded away from 0: 

\[ 
    \cos \theta_k \leqslant -\delta < 0
\]

If we show that, we can then continue with a prove of linear convergence of the Gradient descent from the previous lecture.

Here we show that the constant is bounded: 

\begin{align*}
    \cos \theta_k &= \frac{\nabla f(x_k)^T d_k}{\norm{\nabla f(x_k)} \norm{d_k}} \\ 
    &= \frac{- \nabla f(x_k)^T (\nabla^2 f(x_k))^{-1} \nabla f(x_k)}{\norm{\nabla f(x_k)} \norm{(\nabla^2 f(x_k))^{-1} \nabla f(x_k)}} \\ 
    &\leqslant - \frac{\lambda_{\min}((\nabla^2 f(x_k))^{-1}) \cancel{\norm{\nabla f(x_k)}^2}}{\cancel{\norm{\nabla f(x_k)}^2} \lambda_{\max}((\nabla^2 f(x_k))^{-1})} \\ 
    &= - \frac{\lambda_{\min}}{\lambda_{\max}} \\ 
    &= -\delta < 0 
\end{align*}

\subsubsection{Local convergence rate}

Global convergence means that we start from arbitrary point and converge to the minimum. Local convergence means that we start from a point close to the minimum and converge to the minimum.

\begin{theorem}
    If $f \in \mathcal{C}^{2, 2}_M$, $\nabla^2 f(x_{opt}) \geqslant \mu I, \alpha_k = 1$ then: 
    \[
        \exists r : \; \forall x_0 : \; \norm{x_0 - x_{opt}} \leqslant r
    \]
    and for Newton method: 
    \[ 
        \norm{x_{k+1} - x_{opt}} \leqslant const \cdot \norm{x_k - x_{opt}}^2
    \]

    Step is: 
    \begin{gather*}
        r_{k+1} \leqslant const \cdot r_k^2 \\ 
        \frac{r_{k+1}}{r_k} \leqslant const \cdot \frac{r_k^2}{r_k} \to 0 \Longrightarrow \text{ superlinear convergence}
    \end{gather*}
\end{theorem}
\begin{proof}
    \begin{align*}
        \norm{x_{k+1} - x_{opt}} &= \norm{x_k - H_k^{-1} \nabla f(x_k) - x_{opt}} \\ 
        &= \norm{H_k^{-1} (H_k (x_k - x_{opt}) - (\nabla f(x_k) - \nabla f(x_{opt})))} \\
        &= \norm{H_k^{-1} (H_k (x_k - x_{opt}) - \int_0^1 \nabla^2 f(x_{opt} + \tau (x_k - x_{opt})) (x_k - x_{opt}) d \tau)} \\
        &= \norm{H_k^{-1} \left( \int_0^1 H_k (x_k - x_{opt}) d \tau - \int_0^1 \nabla^2 f(x_{opt} + \tau (x_k - x_{opt})) (x_k - x_{opt}) d \tau \right)} \\
        &= \norm{H_k^{-1} \int_0^1 (\nabla^2 f(x_k) - \nabla^2 f(x_{opt} + \tau (x_k - x_{opt})) (x_k - x_{opt}) d \tau)} \\ 
        &\leqslant \norm{H_k^{-1}} \int_0^1 \norm{\nabla^2 f(x_k) - \nabla^2 f(x_{opt} + \tau (x_k - x_{opt}))} \norm{x_k - x_{opt}} d \tau \\
        &\leqslant \norm{H_k^{-1}} \int_0^1 M (1 - \tau) \norm{x_k - x_{opt}}^2 d \tau \\
        &= \norm{H_k^{-1}} \cdot \frac{M}{2} \cdot \norm{x_k - x_{opt}}^2 \\
    \end{align*}
    The only thing left is to show why the norm of the inverse of the Hessian is bounded.
    \[
        \norm{H_k^{-1}} \leqslant \frac{1}{\mu}
    \]
    It could be shown by the following inequality:
    \[
        H_k = \nabla^2 f(x_{opt}) \geqslant \mu I \Longrightarrow \norm{ \nabla^2 f(x_{k})} \geqslant \frac{\mu}{2} \Longrightarrow \norm{H_k^{-1}} \leqslant \frac{1}{\mu}
    \]
\end{proof}

\subsection{Hessian modifications}

As we already discussed, Newton method sometimes may diverge for non-convex functions due to the Hessian not being positive definite. Tho, we can modify the Hessian to make the method more stable.

General scheme of the method is:
\[
    x_{k+1} = x_k - \alpha_k B^{-1}_k \nabla f(x_k), \quad B_k = \nabla^2 f(x_k) + E_k, \; B_k \succ 0
\]

Where $E_k$ is a symmetric matrix, modifying the Hessian.

We can make several modifications to the Hessian, we will cover couple of them. 

\subsubsection{Regularization}

First things first, we know that Hessian is symmetric by semantics. Meaning that we can do the eigenvalue decomposition (see previous lecture) of the Hessian.

\[ 
    \nabla^2 f(x_k) = Q \Lambda Q^T, \; \Lambda = \text{diag}(\lambda_1, \lambda_2, \dots, \lambda_n)
\] 


What we will do is modify our Hessian so that all the eigenvalues are positive: 
\begin{gather*}
    B_k = Q \tilde{\Lambda} Q^T, \; \tilde{\Lambda} = \begin{cases}
        \lambda_i, \; \text{if } \lambda_i \geqslant \varepsilon \\
        \varepsilon, \; \text{otherwise}
    \end{cases}
\end{gather*}

Symmetric $n \times n$ matrix has $n$ eigenvalues, and these eigenvalues represent how does the matrix, interpreted as a linear transformation, stretch the space. $n$ eigenvalues correspond to $n$ different eigenvectors, and these eigenvectors are orthogonal. They represent different subspaces of the space, and the eigenvalues represent how much the matrix stretches the space in these subspaces.

Let's recall what is an eigenvalue of a matrix $A$: 
\[ 
    A x = \lambda x
\]

If an eigenvalue is positive, then the matrix stretches the space in the direction of the corresponding eigenvector and increases the norm of the vector. If the eigenvalue is negative, then the matrix compresses the space in the direction of the corresponding eigenvector and decreases the norm of the vector. If the eigenvalue is zero, then the matrix doesn't change the space in the direction of the corresponding eigenvector.

Returning to our regularization method. Say Hessian was not positive definite. By definition it will mean that some eigenvalues are non-positive. That means that this point is some kind of saddle point. And we basically don't know, where to move. By regularization we make all the eigenvalues positive, and the method will work as expected.

\subsubsection{Diagonal modification}

\[
    B_k = \nabla^2 f(x_k) + \tau_k I 
\]

Then (as $Q$ is orthogonal, see previous lecture): 
\[ 
    Q \lambda Q^T + \tau_k I = Q \lambda Q^T + \tau_k Q Q^T = Q (\lambda + \tau_k I) Q^T
\]

Like in the previous method, we add some constant to all the eigenvalues. When the constant is large, these eigenvalues will probably become positive. Yet, interpreting all the eigenvalues as really big vallues around infinity is not a good idea as it is interpreted as stretching the space in all the directions with the same magnitude.

Iterations are:
\[
    x_{k+1} = x_k - \alpha_k (\nabla^2 f(x_k) + \tau_k I)^{-1} \nabla f(x_k)
\]

If $\tau_k \gg 1$, then the method is close to the gradient descent (as it also doesn't pay attention to the curvature of the space). If $\tau_k \ll 1$, then the method is close to the Newton method. We use a stable and well-performing method when we can and switch to Newton method when we are close to the minimum.

\begin{algorithm}
    \caption{Diagonal modification Newton method}
    \begin{algorithmic}[1]
    \State For each step, obtain $\nabla^2 f(x_k), \tau_k$
    \While{some condition}
        \State $B_k = \nabla^2 f(x_k) + \tau_k I$
        \State $B_k = L L^T$
        \If{success}
            \State \textbf{exit}
        \EndIf
        \State $\tau_k = \tau_k \cdot \gamma, \;\; \gamma > 1$
    \EndWhile
    \State $\tau_{k+1} = \tau_k \cdot \rho, \;\; \rho < 1$
\end{algorithmic}
\end{algorithm}

Basically we will be making constant smaller and smaller until we find the positive definite matrix. Afterwards we continue with that same constant. And continue to minimize it when Hessian is positive definite and increase it when it is not.

\subsubsection{Modification of LDL decomposition}
The most fancy one, takes advantage of both previous modifications.

\[
    \nabla^2 f(x_k) = P L D L^T P^T
\]

Where $P$ is a permutation matrix, $L$ is a lower triangular matrix, and $D$ is a block diagonal matrix. 

For matrix $D$ let's cosider its eigenvalue decomposition and modify it in the same way as we did in the regularization method: 
\begin{gather*}
    D = Q \Lambda Q^T, \Longrightarrow \tilde{D} = Q \tilde{\Lambda} Q^T \\ 
    B_k = P L \tilde{D} L^T P^T
\end{gather*}

This method is great, however, not used a lot in the standard libraries, because LDL decomposition is not popular in the standard libraries. Yet, it works better then the latter.

\example 
\begin{align*}
    &f(x) = \frac{1}{3} \norm{x}^3 = \frac{1}{3} (x^T x)^{3/2} \\ 
    &df = \frac{1}{3} \frac{3}{2} (x^T x)^{1/2} 2x^T dx \\ 
    &d^2 f = \frac{1}{2} (x^T x)^{-1/2} 2x^T dx_2 x^T dx_1 + (x^T x)^{1/2} dx_2^T dx_1 \\ 
    &\nabla f(x) = \norm{x} x \\
    &\nabla^2 f(x) = \frac{1}{\norm{x}} x x^T + \norm{x} I
\end{align*}

And: 
\begin{align*}
    y &= x - \alpha (\nabla^2 f(x))^{-1} \nabla f(x) \\ 
    &= x - \alpha \left(\frac{1}{\norm{x}} x x^T + \norm{x} T\right)^{-1} \norm{x} x \\ 
    &= x - \alpha \left(\frac{1}{\norm{x}} x x^T + \norm{x} I\right)^{-1} \norm{x} x \\ 
    &= x - \alpha \left(\frac{1}{\norm{x}} I - \frac{1}{\norm{x}} x \left(\norm{x} + x^T \frac{1}{\norm{x}} x\right)^{-1} x^T \frac{1}{\norm{x}}\right) \norm{x} x \\
    &= x - \frac{\alpha}{2} x = (1 - \frac{\alpha}{2}) x
\end{align*}